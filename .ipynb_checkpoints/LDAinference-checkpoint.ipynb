{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import digamma\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from scipy.stats import dirichlet\n",
    "from collections import Counter\n",
    "from random import random\n",
    "import json\n",
    "import heapq\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LDA=LatentDirichletAllocation(n_topics=160,learning_method='online',n_jobs=1,random_state=1,max_iter=500, max_doc_update_iter=1000, perp_tol=1e-3, evaluate_every=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "components=np.load('./data/LDAcomponent.npy')\n",
    "\n",
    "with open('./data/ingredients_dict.json') as json_data:\n",
    "    ingredient_dict = json.load(json_data)\n",
    "    \n",
    "key_file=open('./data/keywords.txt','r')\n",
    "\n",
    "word_list=[]\n",
    "for line in key_file:\n",
    "    word_list.append(line[1:-2].decode('utf-8'))\n",
    "    \n",
    "all_cuisines=filter(lambda x: x.isupper(),word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def marginal_components(comp,word_list,select):\n",
    "    selected_comp=comp[:,np.logical_not(select)]\n",
    "    marginalized_comp=np.sum(comp[:,select],axis=1)\n",
    "    return np.concatenate((selected_comp,marginalized_comp[:,np.newaxis]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def exp_components(comp):\n",
    "    n=comp.shape[1]\n",
    "    return np.exp(digamma(comp)-np.repeat(digamma(np.sum(comp,axis=1))[:,np.newaxis],n,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def input_parse(inputs,word_list, all_cuisine):\n",
    "    cuisine_vector=list(chain.from_iterable([words.split(' ') for words in inputs['cuisine']]))\n",
    "    required_vector=list(chain.from_iterable([words.split(' ') for words in inputs['ingredients_required']]))\n",
    "    excluded_vector=list(chain.from_iterable([words.split(' ') for words in inputs['ingredients_excluded']]))\n",
    "    if cuisine_vector:\n",
    "        excluded_vector.extend(all_cuisine)\n",
    "    word_vector=cuisine_vector+required_vector+excluded_vector\n",
    "    word_dict=defaultdict(int)\n",
    "    for word in cuisine_vector+required_vector:\n",
    "        word_dict[word]+=1\n",
    "    for word in excluded_vector:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word]=0\n",
    "    word_select=lambda x: x in word_vector\n",
    "    word_filtered=filter(word_select, word_list)\n",
    "    word_to_numeric=lambda x: word_dict[x] \n",
    "    word_count=map(word_to_numeric, word_filtered)\n",
    "    word_count.append(10)\n",
    "    return word_filtered, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def words_to_ingredients(word_freq, ingredients):\n",
    "    selected=[]\n",
    "    uncovered=[]\n",
    "    for word in word_freq:\n",
    "        values=ingredients[word]\n",
    "        if len(values)==0:\n",
    "            continue\n",
    "        elif len(values)==1:\n",
    "            selected.append(values[0])\n",
    "        else:\n",
    "            uncovered.extend([word]*word_freq[word])\n",
    "    while len(uncovered)>0:\n",
    "        temp=[]\n",
    "        for word in uncovered:\n",
    "            temp.extend(ingredients[word])\n",
    "        freq=[(k,v,-len(k.split(\" \")),random()) for k, v in Counter(temp).iteritems()]\n",
    "        freq=sorted(freq,key=lambda x:(x[1],x[2],x[3]),reverse=True)\n",
    "        for item in freq:\n",
    "            if item[0] not in selected:\n",
    "                target=item[0]\n",
    "                selected.append(target)\n",
    "                break\n",
    "        for word in target.split(\" \"):\n",
    "            try:\n",
    "                uncovered.remove(word)\n",
    "            except:\n",
    "                pass\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'corn flour', u'red bell pepper', u'nacho cheese tortilla chip', u'salt', u'salsa', u'oil', u'cilantro']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "inputs={'cuisine':['MEXICAN'],'ingredients_required':['pork','pepper'],'ingredients_excluded':['onion','sausage']}\n",
    "key_word, word_count=input_parse(inputs, word_list, all_cuisines)\n",
    "select=np.array([False if word in key_word else True for word in word_list])\n",
    "LDA.components_=marginal_components(components,word_list,select)\n",
    "LDA.exp_dirichlet_component_=exp_components(LDA.components_)\n",
    "LDA.doc_topic_prior_=0.255644474041518 #estimated from original data\n",
    "doc_topic_dist=LDA.transform(word_count)\n",
    "\n",
    "probs=np.empty_like(components)\n",
    "for i in xrange(probs.shape[0]):\n",
    "    probs[i,:]=dirichlet.rvs(alpha=components[i,:])\n",
    "    \n",
    "multinomial=np.squeeze(np.dot(doc_topic_dist,probs))\n",
    "undetermined_words=np.array(word_list)[select]\n",
    "condMultinomial=multinomial[select]\n",
    "words_probs=zip(undetermined_words,condMultinomial)\n",
    "largest_words = dict([(x[0],1) for x in heapq.nlargest(word_count[-1],words_probs,key=itemgetter(1))])\n",
    "results=words_to_ingredients(largest_words, ingredient_dict)\n",
    "\n",
    "print results\n",
    "#inferenced_words=np.random.multinomial(n=word_count[-1], pvals=condMultinomial, size=3)\n",
    "#results=[]\n",
    "#for row in inferenced_words:\n",
    "    #word_freq=zip(undetermined_words, row)\n",
    "    #word_freq=filter(lambda x: x[1]>0, word_freq)\n",
    "    #word_freq=dict(word_freq)\n",
    "    #results.append(words_to_ingredients(word_freq, ingredient_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
